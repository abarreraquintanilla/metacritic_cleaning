---
title: "Práctica 2 - Tipología y ciclo de vida de los datos"
author: "Autores: Adrián Barrios Trujillo, Anna Barrera Quintanilla"
output: html_document
---

Instalamos y cargamos los paquetes necesarios para la ejecución de nuestro script:
```{r setup, echo=TRUE, results='hide'}
packages <- c("rstudioapi", "dplyr", "DescTools", "FactoMineR", "rgl")

for (pkg in packages) {
  if (!requireNamespace(pkg, quietly = TRUE)) {
    install.packages(pkg)
  }
}

library(rstudioapi)
library(stringr)
library(DescTools)
library(FactoMineR)
library(rgl)
library(scatterplot3d)
library(ggplot2)
library(dplyr)
```

## Carga de los datos

Obtenemos el directorio del script actual y lo establecemos como directorio de trabajo

```{r echo=TRUE}
path <- dirname(rstudioapi::getActiveDocumentContext()$path)
setwd(path)
```

Leemos el archivo CSV original, teniendo en cuenta que emplea como separador el punto y coma (";") y posee cabecera:

```{r echo=TRUE}
dataset <- read.csv(paste0(path, "/data/films.csv"), sep=";")
```

## Descripción del dataset

Mostramos a continuación las primeras 5 líneas del dataset leído:

```{r echo=TRUE}
head(dataset)
```
Tras esto, comprobamos el tipo de datos del dataset; para ello, emplearemos tanto str() como summary():

```{r echo=TRUE}
str(dataset)
```
```{r echo=TRUE}
summary(dataset)
```
El dataset "films.csv" contiene 2596 filas repartidas en 14 columnas, y cuyos datos contienen información sobre el top de películas recogidas desde la página web Metacritic (https://www.metacritic.com/). Los campos son: 

* title: título de la película.
* year: año de salida.
* rating: clasificación por edades.
* studio: compañía de producción de la película.
* duration: duración en horas y minutos (formato X h YY m).
* must_see: etiqueta "MUST-SEE" de Metacritic como recomendación de la página web.
* metascore: puntuación recibida por los críticos de Metacritic.
* user_score: puntuación recibida por los usuarios de la página.
* number_critics: número de críticos que han calificado la película.
* number_users: número de usuarios que han calificado la película.
* genre: género de la película.
* director: director/a o directores de la película.
* writer: escritor/a o escritores de la película.
* summary: resumen del argumento de la película.

Nuestro objetivo es analizar qué películas reciben la etiqueta "MUST-SEE" (campo 'must_see') de Metacritic, empleando las otras características presentes en el dataset. Para ello, haremos uso de la información recogida previamente con str() y summary() para comprobar el estado de los datos y prepararlos para los algoritmos que usaremos en nuestro análisis.

Primero, podemos observar que hay algunos campos que presentan valores nulos, como 'year' o 'number_users'; por tanto, tendremos que lidiar con ello en la limpieza que realizaremos más adelante, según la naturaleza de cada uno de ellos.

También hemos visto que hay campos de valor numérico que tienen asignado el tipo "character" o que son de tipo 'numeric' cuando podrían ser de tipo 'integer'; por tanto, es recomendable pasarlos a un tipo más adecuado según corresponda. Estos campos son: 'user-score' (es tipo 'character' cuando debería ser 'integer') y 'number_users' (es de tipo 'numeric', y lo pasaremos a 'integer'). Además, los campos numéricos, en caso de ser usados más adelante, deberán escalarse para un mejor análisis.

Respecto a las variables categóricas, tendríamos que revisar sus valores para ver si podríamos codificarlas de manera que podamos aplicar los algoritmos sin que presenten muchas dificultades o, si por el contrario, sería mejor no emplearlas.

Por último, la variable 'must_see' es de tipo 'character', pero sus valores son de tipo booleano, es decir, "True" y "False". Emplearemos este campo como variable objetivo por lo que, para facilitar su uso, también codificaremos este campo en el siguiente apartado.

## Integración y selección de los datos de interés a analizar

En nuestro análisis, hemos considerado que algunas variables podrían presentar valores poco útiles o generar demasiado ruido en nuestro modelos. En este caso, hacemos referencia a una serie de campos categóricos que, como explicaremos más adelante, dificultarían nuestro análisis. El listado es el siguiente:

* title
* studio
* director
* writer
* summary

Para empezar, 'title' se refiere al nombre de cada película, por lo que no es una característica que nos pueda servir para encontrar patrones o rasgos comunes entre ellas; lo mismo sucede con la descripción o resumen ('summary'), que debería ser única para cada película. Comprobemos el total de valores únicos para cada uno de estos campos, que debería ser igual o similar (si hay filas o valores repetidos puede afectar a este número) al total de observaciones (2596):

El número de títulos distintos es:
```{r echo=TRUE}
dataset$title %>% unique() %>% length()
```
El número de títulos repetidos es:
```{r echo=TRUE}
dataset$title[duplicated(dataset$title)] %>% unique() %>% length()
```
El número de resúmenes distintos es:
```{r echo=TRUE}
dataset$summary %>% unique() %>% length()
```
El número de resúmenes repetidos es:
```{r echo=TRUE}
dataset$summary[duplicated(dataset$summary)] %>% unique() %>% length()
```
Tanto el número de títulos como el número de resúmenes es prácticamente igual al total de observaciones, y aunque podemos ver que hay algunos datos repetidos en ambos campos, nuestro argumento sigue siendo el mismo: son demasiadas características diferentes como para poder detectar alguna semejanza entre las películas. 

Respecto al resto, 'studio', 'director' y 'writer' presentan también una gran cantidad de combinaciones diferentes que dificultarían nuestra labor:

El número de estudios distintos es:
```{r echo=TRUE}
dataset$studio %>% unique() %>% length()
```
El número de directores distintos es:
```{r echo=TRUE}
dataset$director %>% paste(., collapse = ",") %>% strsplit(",") %>% unlist() %>% trimws() %>% unique() %>% length()
```

El número de escritores distintos es:
```{r echo=TRUE}
dataset$writer %>% paste(., collapse = ",") %>% strsplit(",") %>% unlist() %>% trimws() %>% unique() %>% length()
```

Para empezar, 'studio' contiene 459 datos únicos, es decir, hay 459 compañías diferentes encargadas de producir las películas de este dataset; dicho número sigue siendo lo suficientemente grande como para dificultar el rendimiento de nuestros algoritmos.

Si hablamos de los directores y escritores implicados, hemos aislado primero el nombre de cada uno, dado que podían haber uno o varios (incluso ninguno) para cada película, y luego hemos comprobado el número de valores únicos; pese a ello, seguían siendo demasiados: 1760 directores diferentes y 3697 escritores.

Por tanto, dada la gran cantidad de distintos valores presentes en cada una de estas columnas, las eliminaremos, de forma que faciliten nuestro uso:

```{r echo=TRUE}
dataset <- dataset %>% dplyr::select(-title, -studio, -director, -writer, -summary)
dataset %>% str()
```

## Limpieza de datos

Comenzamos este apartado con la limpieza de los valores nulos: recordemos que 'year' y 'number_users' ya nos presentaron antes este tipo de valores. Para 'year' podemos ver que son solo 24 valores nulos; por tanto, al ser una cantidad lo suficientemente pequeña, podemos eliminar estas filas de nuestro dataset.

```{r echo=TRUE}
cleanData <- dataset[!is.na(dataset$year),]
```

Sin embargo, 'number_users' presenta un mayor número de valores NA: un total de 488 observaciones. Por tanto, tendremos que analizar previamente su impacto junto al campo 'user_score', dada la relación entre ambos:

```{r echo=TRUE}
dataUsers <- cleanData %>% dplyr::filter(is.na(cleanData$number_users)) %>% dplyr::select(user_score, number_users)
dataUsers %>% head()
```
Número de filas presentes en dataUsers, es decir, número de observaciones que tienen valor NA en 'number_users':
```{r echo=TRUE}
dataUsers %>% nrow()
```
Puntuaciones únicas cuando number_users es NA:
```{r echo=TRUE}
dataUsers$user_score %>% table()
```
Puntuaciones únicas en el dataset limpio (hasta este paso):
```{r echo=TRUE}
cleanData$user_score %>% table()
```
Podemos comprobar que, cuando el número de usuarios ('number_users') posee valor NA, siguen existiendo puntuaciones asociadas a los usuarios; esto puede deberse a que la página no haya registrado el número de usuarios implicados en la valoración de la película, o que nuestra recogida de datos no ha extraído correctamente esta información; aparte, hay 380 observaciones que no poseen una puntuación indicada (tbd o "to be discussed") - también hemos comprobado si esto ocurre en el dataset limpiado hasta este punto (sin las películas con 'year' igual a NA), y siguen siendo 380 observaciones. 

Ante esta situación, hemos decidido asignar, por un lado, el valor 1 para 'number_users' en aquellas filas donde aparezca un valor distinto a "tbd"; por otro lado, el valor "tbd" de 'user_score' se intercambiará por el valor 0 para todos los casos, mientras que el campo 'number_users' será 0 cuando sea NA, dado que al seguir siendo bastantes datos (464 de 2572), su eliminación podría afectar a nuestro análisis.

```{r echo=TRUE}
cleanData$user_score[cleanData$user_score == "tbd"] <- 0.0
cleanData$number_users[is.na(cleanData$number_users)] <- 0
cleanData$user_score %>% table() %>% head()
cleanData$number_users %>% table() %>% head()
```

Para terminar con estos campos, pasaremos 'number_users' de "numeric" a "integer", es decir, cambiarlo de número decimal a número entero, dado que el número de usuarios no debería poder aceptar valores decimales; por otro lado, 'user_score' pasará de "character" a "numeric", lo multiplicaremos por 10 para dejarlo sobre 100 al igual que 'metascore', y se convertirá a "integer" para trabajar con enteros en este campo:

```{r echo=TRUE}
cleanData$number_users <- cleanData$number_users %>% as.integer()
cleanData$user_score   <- cleanData$user_score %>% as.numeric() %>% `*`(10) %>% as.integer()
```

Tras la eliminación de las variables categóricas presentadas en el apartado de "Integración y selección de los datos de interés a analizar", comprobaremos si el resto presentan como valor la cadena vacía (""), y según la frecuencia de dicho valor, nos plantearemos si eliminar dichas filas o sustituirlas por un valor distinto:

```{r echo=TRUE}
colsToCheck <- c("rating", "duration", "must_see", "genre")
numberEmpty <- numeric(length(colsToCheck))

for (i in seq_along(colsToCheck)) {
  numberEmpty[i] <- sum(cleanData[[colsToCheck[i]]] == "")
}

names(numberEmpty) <- colsToCheck
print("Número de cadenas vacías ('') en las columnas categóricas:")
print(numberEmpty)
```
Podemos comprobar que existen cadenas vacías en las variables categóricas, por lo que, comenzando por 'rating', analizaremos sus valores y frecuencias para comprobar esas 325 observaciones que están vacías, así como otras consideraciones que puedan surgir durante el procesado y debamos tener en cuenta:

```{r echo=TRUE}
cleanData$rating %>% table() %>% sort(decreasing = T)
```

Se observa que hay multitud de calificaciones, desde clasificaciones de películas hasta clasificaciones propias de series de televisión (por ejemplo, TV-14), así como valores vacíos o equivalentes ("Not Rated"). Ante esta situación, y para facilitar su uso posterior, codificaremos esta variable combinando las clasificaciones por películas (ver referencia: https://www.motionpictures.org/film-ratings/) como de series de televisión (ver referencia: https://www.thetvboss.org/tv-ratings/); en caso de ser un valor que no esté incluido en ambas, se asignará un valor nuevo dependiendo del tipo que sea. La lista de codificación a una variable numérica será la siguiente:

* Valor 0: Not Rated, "", Unrated 
* Valor 1: G, TV-Y, TV-Y7, TV-Y7-FV, TV-G
* Valor 2: PG, TV-PG
* Valor 3: PG-13, TV-14
* Valor 4: R
* Valor 5: NC-17, TV-MA
* Valor 6: El resto de clasificaciones

Somos conscientes de que esto puede simplificar su categorización más de lo necesario, pero mejorará su uso para los algoritmos que apliquemos más tarde. Dicho esto, realizaremos el cambio a continuación - pero cabe decir que, primero, cambiaremos el valor "" por "Empty" y "Not Rated" por "Not_Rated" para poder emplear la función recode() sin errores:

```{r echo=TRUE}
cleanData$rating <- ifelse(cleanData$rating == "", "Empty", cleanData$rating)
cleanData$rating <- ifelse(cleanData$rating == "Not Rated", "Not_Rated", cleanData$rating)
cleanData$rating <- dplyr::recode(cleanData$rating,
                           "Not_Rated" = 0, "Empty" = 0, "Unrated" = 0,
                           "G"  = 1, "TV-Y"  = 1, "TV-Y7" = 1, "TV-Y7-FV" = 1, "TV-G" = 1,
                           "PG" = 2, "TV-PG" = 2,
                           "PG-13" = 3, "TV-14" = 3,
                           "R" = 4,
                           "NC-17" = 5, "TV-MA" = 5,
                           .default = 6) %>% as.integer()
```

Valores absolutos tras la codificación:
```{r echo=TRUE}
cleanData$rating %>% table()
```
Pesos por porcentajes tras la codificación:
```{r echo=TRUE}
cleanData$rating %>% table() %>% prop.table()*100
```
Lo siguiente es analizar el campo 'duration': esta variable recoge la duración de la película en un formato "XXh YYm", pudiendo no presentar el campo de la hora si la película es lo suficientemente corta. Además, solo presenta 54 filas cuyos valores estén vacíos, por lo que podamos eliminar dichas filas de nuestro análisis, y aprovecharemos por eliminar también los valores NA que podamos haber no detectado en este proceso de limpieza:
```{r echo=TRUE}
cleanData <- cleanData %>% filter(!is.na(duration) & duration != "")
```

El siguiente paso convertir la duración de las películas en un formato que sea más accesible, por lo que obtendremos el total de minutos de cada película en formato integer. Para ello, extraeremos la hora (si existe), la multiplicaremos por 60, y la sumaremos con el total de minutos, sustituyendo dicho resultado donde el valor original se encontraba:
```{r echo=TRUE}
cleanData <- cleanData %>% mutate(
                              duration = gsub(" ", "", duration),
                              duration = ifelse(is.na(str_extract(duration, "\\d+(?=h)")), 0, as.integer(str_extract(duration, "\\d+(?=h)"))) * 60 + as.integer(str_extract(duration,"\\d+(?=m)")))
```

Tras esto, comprobamos cómo son las estadísticas básicas de esta variable, ahora de tipo numérica:
```{r echo=TRUE}
cleanData %>%
    summarise(
        count = n(),                                 # Número de observaciones
        mean = mean(duration, na.rm = TRUE),         # Media
        sd = sd(duration, na.rm = TRUE),             # Desviación estándar
        min = min(duration, na.rm = TRUE),           # Mínimo
        q1 = quantile(duration, 0.25, na.rm = TRUE), # 1er cuartil
        median = median(duration, na.rm = TRUE),     # Mediana
        q3 = quantile(duration, 0.75, na.rm = TRUE), # 3er cuartil
        max = max(duration, na.rm = TRUE)            # Máximo
    )
```
Hemos podido comprobar que la película más corta dura 30 minutos, mientras que la más larga se extiende hasta los 776 minutos (tal duración puede deberse a que haya sido realmente una serie de televisión clasificada como película). Para facilitar nuestro análisis, dividiremos y codificaremos las películas según los cuartiles; es decir, si una película es superior a 30 minutos pero inferior a 93.25, se considerará muy corta; además, como la diferencia entre el tercer cuartil y el máximo es de gran magnitud, añadiremos un límite más que será el valor intermedio entre el q3 y el máximo: (776-121)/2= 327.5 ~ 328. El listado es el siguiente:

* Valor 0 (muy corta): 30 <= x < 93 
* Valor 1 (corta): 93 <= x < 105
* Valor 2 (normal): 105 <= x < 121
* Valor 3 (larga): 121 <= x < 328
* Valor 4 (muy larga): 328 <= x <= 776

```{r echo=TRUE}
cleanData$duration <- cut(cleanData$duration, breaks = c(30, 93, 105, 121, 328, 776), labels = FALSE, right = FALSE) -1 # -1 para empezar desde 0
cleanData$duration <- cleanData$duration %>% as.integer()
cleanData$duration %>% table()
```
El siguiente campo a analizar es el género, el cual podemos observar que puede presentar muchas combinaciones:
```{r echo=TRUE}
cleanData$genre %>% head()
cleanData$genre %>% unique() %>% length()
```
Esto se debe a que pueden haber multitud de combinaciones de géneros; para facilitar nuestro análisis, comprobemos primero qué géneros existen de manera individual y con qué frecuencia aparecen:

```{r echo=TRUE}
cleanData$genre %>% paste(., collapse = ",") %>% strsplit(",") %>% unlist() %>% trimws() %>% table() %>% sort(decreasing = T)
cat("Número de géneros diferentes: ", cleanData$genre %>% paste(., collapse = ",") %>% strsplit(",") %>% unlist() %>% trimws() %>% table() %>% length())
```
Podemos observar que hay 25 géneros distintos, entre los que se incluyen géneros desconocidos ('Unknow'). Sin embargo, nuestro problema radica en que cada película puede poseer distintos géneros que la definan, así como un número variable de géneros, como se muestra a continuación:

Frecuencia de números de géneros que poseen las películas:
```{r echo=TRUE}
cleanData$genre %>% strsplit(",") %>% sapply(length) %>% table()
```
Se ha decidido crear diferentes columnas dentro del dataset de tipo "boleano" para indicar cada uno de los generos. Lo hacemos comprobando para cada una de las muestras si existe el genero(string) y asignando 0 o 1 en función si lo encuentra o no. Como eran muchos generos, se ha decidido mostrar solo los más frecuentes y el resto asignarlos a "others". 
Así, podremos analizar con más facilidad sobre este atributo:

```{r echo=TRUE}
cleanData$genre_drama       <- ifelse(grepl("Drama", cleanData$genre) == "TRUE", 1, 0) %>% as.integer()
cleanData$genre_crime       <- ifelse(grepl("Crime", cleanData$genre) == "TRUE", 1, 0) %>% as.integer()
cleanData$genre_mystery     <- ifelse(grepl("Mystery", cleanData$genre) == "TRUE", 1, 0) %>% as.integer()
cleanData$genre_romance     <- ifelse(grepl("Romance", cleanData$genre) == "TRUE", 1, 0) %>% as.integer()
cleanData$genre_adventure   <- ifelse(grepl("Adventure", cleanData$genre) == "TRUE", 1, 0) %>% as.integer()
cleanData$genre_biography   <- ifelse(grepl("Biography", cleanData$genre) == "TRUE", 1, 0) %>% as.integer()
cleanData$genre_thriller    <- ifelse(grepl("Thriller", cleanData$genre) == "TRUE", 1, 0) %>% as.integer()
cleanData$genre_comedy      <- ifelse(grepl("Comedy", cleanData$genre) == "TRUE", 1, 0) %>% as.integer()
cleanData$genre_history     <- ifelse(grepl("History", cleanData$genre) == "TRUE", 1, 0) %>% as.integer()
cleanData$genre_documentary <- ifelse(grepl("Documentary", cleanData$genre) == "TRUE", 1, 0) %>% as.integer()
cleanData$genre_others      <- ifelse(grepl("Action|Game-Show|News|Film-Noir|Western|Musical|Sport|Horror|Sci-Fi|Animation|Family|Action|Fantasy|War|Music", cleanData$genre) == "TRUE", 1, 0) %>% as.integer()

```

A continuación eliminamos la columna "genre" porque ya no nos hace falta:
```{r echo=TRUE}
cleanData <- cleanData %>% dplyr::select(-genre)
```

```{r echo=TRUE}
str(cleanData)
```
El siguiente paso será la codificación de la variable objetivo, donde "True" será 1, y "False" será 0. De esta forma, podemos emplear estos nuevos valores con algoritmos que requieran valores numéricos, entre otras ventajas:

```{r echo=TRUE}
cleanData$must_see <- ifelse(cleanData$must_see == "True", 1, 0) %>% as.integer()
```

Por último, comprobaremos cuántas filas repetidas existen en el dataset y las eliminaremos, como se puede apreciar a continuación:

```{r echo=TRUE}
cat("Número de filas totales: ", cleanData %>% nrow())
cat("Número de filas repetidas: ", cleanData %>% duplicated() %>% sum())
cleanData <- cleanData %>% filter(!duplicated(.))
```

Estado final del dataset:
```{r echo=TRUE}
str(cleanData)
```
```{r echo=TRUE}
summary(cleanData)
```
```{r echo=TRUE}
head(cleanData)
```

El dataset con los datos ya limpios se exportará como un nuevo CSV, como se muestra a continuación:
```{r echo=TRUE}
write.csv2(cleanData, file = paste0(path, "/data/films_clean.csv"), row.names = FALSE)
```

## Análisis de los datos

### Comprobación de la normalidad

Comprobamos la normalidad de las variables con qqplot y histogramas:

```{r echo=TRUE}
par(mfrow=c(2,2))
for(i in 1:ncol(cleanData)) {
  if (is.numeric(cleanData[,i])){
    qqnorm(cleanData[,i],main = paste("Normal Q-Q Plot for",colnames(cleanData)[i]))
    qqline(cleanData[,i],col="red")
    hist(cleanData[,i], 
      main=paste("Histogram for", colnames(cleanData)[i]), 
      xlab=colnames(cleanData)[i], freq = FALSE)
  }
}
``` 
Los resultados del quantile-quantile plot nos indica que algunas variables pueden ser candidatas a la normalización si es necesario. Por ejemplo, las variables tipo factor (1/0) no tiene sentido normalizarlas y las codificadas tampoco.

Entonces nos quedariamos con: year, metascore, user_score, number_critics y number_users.

### Prueba de contraste de hipótesis de Shapiro-Wilk

Para revisar si las variables están normalizadas se aplica el test de Shapiro-Wilk en cada variables numérica de las antes mencionadas:

```{r ,eval=TRUE,echo=TRUE}
shapiro.test(cleanData$year)
```
```{r ,eval=TRUE,echo=TRUE}
shapiro.test(cleanData$metascore)
```

```{r ,eval=TRUE,echo=TRUE}
shapiro.test(cleanData$user_score)
```

```{r ,eval=TRUE,echo=TRUE}
shapiro.test(cleanData$number_critics)
```

```{r ,eval=TRUE,echo=TRUE}
shapiro.test(cleanData$number_users)
```

El test nos indica que ninguna variable esta normalizada, ya que el p-valor es inferior al coeficiente 0.05, por lo que se puede rechazar la hipotesis nula y entender que no es normal.

Que no sea normal no quiere decir que no pueda ser normalizable, ya que según el teorema del límite central, al tener más de 30 elementos en las observaciones, podemos aproximarla como una distribución normal de media 0 y desviación estándar 1.

```{r ,eval=TRUE,echo=TRUE}
year.norm <- BoxCox(cleanData$year, lambda = BoxCoxLambda(cleanData$year))
shapiro.test(year.norm)
```
```{r ,eval=TRUE,echo=TRUE}
qqnorm(year.norm)
qqline(year.norm, col=2)
```

En el caso de year, no mejora nada.

Vamos a probar con metascore:
```{r ,eval=TRUE,echo=TRUE}
metascore.norm <- BoxCox(cleanData$user_score, lambda = BoxCoxLambda(cleanData$user_score))
shapiro.test(metascore.norm)
```
```{r ,eval=TRUE,echo=TRUE}
qqnorm(metascore.norm)
qqline(metascore.norm, col=2)
```
Tampoco obtenemos mejores resultados de los ya tratados.

Se pueden probar tantas transformaciones como se desee, pero en este caso supondremos que los datos no se pueden normalizar y, por tanto, los analizaremos con pruebas que no presuponen estas caracteristicas.

### Prueba de contraste de hipótesis de Fligner-Killeeen

En referencia a la homogeneidad de la varianza, dado que los datos no son normales, usaremos el método de Fligner-Killeen, para comparar la varianza entre las puntuaciones y las variables factor:

```{r ,eval=TRUE,echo=TRUE}
fligner.test(metascore ~ must_see, data = cleanData)
```
Para metascore, la varianza es similar entre pelis buenas y malas.

```{r ,eval=TRUE,echo=TRUE}
fligner.test(metascore ~ rating, data = cleanData)
```
En cambio, es diferente entre los grupos de rating.

### Prueba de contraste de hipótesis de Wilcoxon

Dado que los datos a analizar no presentan una distribución normal, las pruebas estadísticas para analizar y comparar los diferentes grupos deberán ser no paramétricas. Así, para los datos como los géneros que son booleanos, se aplica el test de Wilcoxon:

Probamos con drama:
```{r ,eval=TRUE,echo=TRUE}
wilcox.test(cleanData$metascore~cleanData$genre_drama)
```
Probamos con comedy:
```{r ,eval=TRUE,echo=TRUE}
wilcox.test(cleanData$metascore~cleanData$genre_comedy)
```
Por último, probamos con crime:
```{r ,eval=TRUE,echo=TRUE}
wilcox.test(cleanData$metascore~cleanData$genre_crime)
```
Podemos observar con la primera, que la diferencia entre los grupos es significativa (p<0.05). Es decir, el factor metascore es diferente cuando es la pelicula de drama o no, puede haber una relación.
En cambio, cuando hemos aplicado el mismo modelo, pero con comedy y crime, vemos el factor p>0.05.

El test equivalente cuando se tienen 3 o más grupos de datos, como el caso de rating o duration, es el test de Kruskal-Wallis:
```{r ,eval=TRUE,echo=TRUE}
kruskal.test(cleanData$metascore~cleanData$rating)
```
Son estadisticamente diferentes entre ellos.

Vamos a comprobar la correlación, a partir del método Spearman:

```{r ,eval=TRUE,echo=TRUE}
cor.test(cleanData$genre_drama, cleanData$must_see,method="spearman")
```
No vemos mucha correlación.

### Modelo Supervisado

Vamos a probar con una regresión lineal simple, comparando la variable factor must_see con el número de criticos:

```{r ,eval=TRUE,echo=TRUE}
data_glm<-cleanData[]

ntrain <- nrow(data_glm)*0.8
ntest <- nrow(data_glm)*0.2
set.seed(1)
index_train<-sample(1:nrow(data_glm),size = ntrain)
train<-data_glm[index_train,]
test<-data_glm[-index_train,]
modelo<-lm(formula = must_see ~ ., data=train)
summary(modelo)
```
```{r , eval=TRUE,echo=TRUE}
cols <- setdiff(names(train), "must_see")

par(mfrow = c(2, 3)) 

for (col in cols) {
  plot(train[, col], train$must_see, main = paste0(col, " vs must_see"), xlab = col, ylab = "must_see", col = "blue", pch = 20)
  abline(lm(train$must_see ~ train[, col]), col = "red")
}
```

Gracias a estos gráficos, podemos determinar qué tipo de correlación posee cada campo con 'must_see'. Por ejemplo, es lógico que, a mayor 'metascore', es decir, a mayor calificación de los críticos de la página de metacritic, mayor sea la probabilidad de que se etiquete como 'must_see', llegando al caso de que, si la película posee una puntuación de aproximadamente 95 o más, es casi seguro que reciba tal clasificación. 

Otros casos que nos han parecido interesantes han sido que, conforme avancen los años, 'must_see' se reduzca; en otras palabras, existe una correlación negativa con 'year'. Esto puede deberse a que la página ha recogido solo las mejores películas de los primeros años de cinematografía, que la calidad de las películas se haya reducido con el tiempo, que la crítica sea más "dura" con las nuevas películas, que dicha calificación necesite un periodo mínimo para poder recomendarse con esta etiqueta, etc.

### Modelo no supervisado

Usaremos clusters, para ello primero aplicaremos el codo de Jambú par escoger el número de clústeres que minimice la distancia intra-cluster:
```{r ,eval=TRUE,echo=TRUE}
dataset_clu <- cleanData %>% dplyr::select(number_critics, must_see)
```

```{r ,eval=TRUE,echo=TRUE}
ss = 1:20
for( i in 1:20){
  modelo = kmeans(x = dataset_clu, centers = i, iter.max = 20)
  ss[[i]] = modelo$tot.withinss
}
plot(1:20, ss, xlab = "Clusters", ylab = "Distancia intra clusters total",type = "b")
```

Los valores k=4 o k=5 serían una buena elección, dado que, a partir de ellos, los cambios no son significativos.

Quitamos las variables factor:
```{r ,eval=TRUE,echo=TRUE}
summary(cleanData)
```
```{r ,eval=TRUE,echo=TRUE}
pairs(cleanData[,-c(4,9,10,11,12,13,14,15,16,17,18,19)], col=cleanData$must_see)
```

```{r ,eval=TRUE,echo=TRUE}
pca = PCA(cleanData[,-c(4,9,10,11,12,13,14,15,16,17,18,19)])
```


Al realizar el gráfico 2 a 2 de las variables del set de datos, y además, aplicando un análisis de componentes principales, nos podemos dar cuenta, que los datos parecen no ser separables. Por lo tanto, no existe alguna manera de detectar con una precisión aceptable, los elementos de una clase o de otra.

Aplicaremos un modelo de K-Medias, para observar los resultados y analizar el rendimiento luego, con el fin de dar soporte a lo anterior. Pero solo con las variables year, metascore y number_critics, porque las otras contienen ceros y este modelo no lo soporta:

```{r ,eval=TRUE,echo=TRUE}
cleanData.model = kmeans(cleanData[,-c(2,4,3,6,8,9,10,11,12,13,14,15,16,17,18,19)], centers=2, iter.max=20)
plot(cleanData[,-c(2,3,4,6,8,9,10,11,12,13,14,15,16,17,18,19)], col=cleanData.model$cluster)
```

Aquí se observa, por ejemplo, que cuanto más nueva es la película mayor es el número de críticos que la puntúan. Tiene sentido, ya que en las antiguas seguramente no había tantas opiniones informatizadas.

Evaluación del modelo:

```{r ,eval=TRUE,echo=TRUE}
cleanData.conf = table(cleanData$must_see, cleanData.model$cluster, dnn=c("MustSee", "Cluster"))
cleanData.conf
```
La tabla de contingencia (o matrix de confusión) proporciona información detallada sobre cómo las predicciones del modelo se comparan con los valores reales de la variable objetivo 'musts_see'. Por ejemplo, la celda (0, 1) nos dice que el modelo clasificó correctamente 1057 observaciones como pertenecientes al clúster 1 cuando su valor real en la variable objetivo era 0; sin embargo, la celda (0, 2) muestra que el modelo clasificó erróneamente 235 observaciones como pertenecientes al clúster 2 cuando su valor real era 0. 

De manera similar, la celda (1, 1) indica que el modelo clasificó incorrectamente 960 observaciones como pertenecientes al clúster 1 cuando su valor real era 1, indicando que tenía la etiqueta "Must See", y la celda (1, 2) señala que el modelo acertó al clasificar 261 observaciones como pertenecientes al clúster 2 cuando su valor real era 1. 

Por tanto, podemos decir que el modelo tiene una buena precisión, pero es bastante mejorable respecto a las instancias negativas.

## Conclusiones 

Hemos podido comprobar que a mayor metascore mayor probabilidad de must_see, que es esperable y cuando mayor número de usuarios que han puntuado también mayor must_see.

Otro caso curioso, cuanto más dura la película hay una correlación positiva, probablemente sea más buena la película. 

Y respecto a los géneros, si la película es de género drama, crimen, misterio, aventura o thriller tiene una ligera correlación positiva, mayor que en el resto. En cambio, en los documentales es negativa. 

Y por último, el año, presenta correlación negativa, en cuanto más nueva sea la película menos probabilidad hay de que sea un must see.