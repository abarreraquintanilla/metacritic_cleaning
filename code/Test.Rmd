---
title: "Práctica 2 - Tipología y ciclo de vida de los datos"
author: "Autores: Adrián Barrios Trujillo, Anna Barrera Quintanilla"
output: html_document
---

Instalamos y cargamos los paquetes necesarios para la ejecución de nuestro script:
```{r setup, echo=TRUE, results='hide'}
if (!requireNamespace("rstudioapi", quietly = TRUE)) {
  install.packages("rstudioapi")
}

if (!requireNamespace("dplyr", quietly = TRUE)) {
  install.packages("dplyr")
}

library(rstudioapi)
library(dplyr)
library(stringr)
```

## Carga de los datos

Obtenemos el directorio del script actual y lo establecemos como directorio de trabajo

```{r echo=TRUE}
path <- dirname(rstudioapi::getActiveDocumentContext()$path)
setwd(path)
```

Leemos el archivo CSV original, teniendo en cuenta que emplea como separador el punto y coma (";") y posee cabecera:

```{r echo=TRUE}
dataset <- read.csv("../data/films.csv" ,sep=";")
```

## Descripción del dataset

Mostramos a continuación las primeras 5 líneas del dataset leído:

```{r echo=TRUE}
head(dataset)
```
Tras esto, comprobamos el tipo de datos del dataset; para ello, emplearemos tanto str() como summary():

```{r echo=TRUE}
str(dataset)
```
```{r echo=TRUE}
summary(dataset)
```
El dataset "films.csv" contiene 2596 filas repartidas en 14 columnas, y cuyos datos contienen información sobre el top de películas recogidas desde la página web Metacritic (https://www.metacritic.com/). Los campos son: 

* title: título de la película.
* year: año de salida.
* rating: clasificación por edades.
* studio: compañía de producción de la película.
* duration: duración en horas y minutos (formato X h YY m).
* must_see: etiqueta "MUST-SEE" de Metacritic como recomendación de la página web.
* metascore: puntuación recibida por los críticos de Metacritic.
* user_score: puntuación recibida por los usuarios de la página.
* number_critics: número de críticos que han calificado la película.
* number_users: número de usuarios que han calificado la película.
* genre: género de la película.
* director: director/a o directores de la película.
* writer: escritor/a o escritores de la película.
* summary: resumen del argumento de la película.

Nuestro objetivo es analizar qué películas reciben la etiqueta "MUST-SEE" (campo 'must_see') de Metacritic, empleando las otras características presentes en el dataset. Para ello, haremos uso de la información recogida previamente con str() y summary() para comprobar el estado de los datos y prepararlos para los algoritmos que usaremos en nuestro análisis.

Primero, podemos observar que hay algunos campos que presentan valores nulos, como 'year' o 'number_users'; por tanto, tendremos que lidiar con ello en la limpieza que realizaremos más adelante, según la naturaleza de cada uno de ellos.

También hemos visto que hay campos de valor numérico que tienen asignado el tipo "character" o que son de tipo 'numeric' cuando podrían ser de tipo 'integer'; por tanto, es recomendable pasarlos a un tipo más adecuado según corresponda. Estos campos son: 'user-score' (es tipo 'character' cuando debería ser 'integer') y 'number_users' (es de tipo 'numeric', y lo pasaremos a 'integer'). Además, los campos numéricos, en caso de ser usados más adelante, deberán escalarse para un mejor análisis.

Respecto a las variables categóricas, tendríamos que revisar sus valores para ver si podríamos codificarlas de manera que podamos aplicar los algoritmos sin que presenten muchas dificultades o, si por el contrario, sería mejor no emplearlas.

Por último, la variable 'must_see' es de tipo 'character', pero sus valores son de tipo booleano, es decir, "True" y "False". Emplearemos este campo como variable objetivo por lo que, para facilitar su uso, también codificaremos este campo en el siguiente apartado.

## Integración y selección de los datos de interés a analizar

En nuestro análisis, hemos considerado que algunas variables podrían presentar valores poco útiles o generar demasiado ruido en nuestro modelos. En este caso, hacemos referencia a una serie de campos categóricos que, como explicaremos más adelante, dificultarían nuestro análisis. El listado es el siguiente:

* title
* studio
* director
* writer
* summary

Para empezar, 'title' se refiere al nombre de cada película, por lo que no es una característica que nos pueda servir para encontrar patrones o rasgos comunes entre ellas; lo mismo sucede con la descripción o resumen ('summary'), que debería ser única para cada película. Comprobemos el total de valores únicos para cada uno de estos campos, que debería ser igual o similar (si hay filas o valores repetidos puede afectar a este número) al total de observaciones (2596):

El número de títulos distintos es:
```{r echo=TRUE}
dataset$title %>% unique() %>% length()
```
El número de títulos repetidos es:
```{r echo=TRUE}
dataset$title[duplicated(dataset$title)] %>% unique() %>% length()
```
El número de resúmenes distintos es:
```{r echo=TRUE}
dataset$summary %>% unique() %>% length()
```
El número de resúmenes repetidos es:
```{r echo=TRUE}
dataset$summary[duplicated(dataset$summary)] %>% unique() %>% length()
```
Tanto el número de títulos como el número de resúmenes es prácticamente igual al total de observaciones, y aunque podemos ver que hay algunos datos repetidos en ambos campos, nuestro argumento sigue siendo el mismo: son demasiadas características diferentes como para poder detectar alguna semejanza entre las películas. 

Respecto al resto, 'studio', 'director' y 'writer' presentan también una gran cantidad de combinaciones diferentes que dificultarían nuestra labor:

El número de estudios distintos es:
```{r echo=TRUE}
dataset$studio %>% unique() %>% length()
```
El número de directores distintos es:
```{r echo=TRUE}
dataset$director %>% paste(., collapse = ",") %>% strsplit(",") %>% unlist() %>% trimws() %>% unique() %>% length()
```

El número de escritores distintos es:
```{r echo=TRUE}
dataset$writer %>% paste(., collapse = ",") %>% strsplit(",") %>% unlist() %>% trimws() %>% unique() %>% length()
```

Para empezar, 'studio' contiene 459 datos únicos, es decir, hay 459 compañías diferentes encargadas de producir las películas de este dataset; dicho número sigue siendo lo suficientemente grande como para dificultar el rendimiento de nuestros algoritmos.

Si hablamos de los directores y escritores implicados, hemos aislado primero el nombre de cada uno, dado que podían haber uno o varios (incluso ninguno) para cada película, y luego hemos comprobado el número de valores únicos; pese a ello, seguían siendo demasiados: 1760 directores diferentes y 3697 escritores.

Por tanto, dada la gran cantidad de distintos valores presentes en cada una de estas columnas, las eliminaremos, de forma que faciliten nuestro uso:

```{r echo=TRUE}
dataset <- dataset %>% select(-title, -studio, -director, -writer, -summary)
dataset %>% str()
```

## Limpieza de datos

Comenzamos este apartado con la limpieza de los valores nulos: recordemos que 'year' y 'number_users' ya nos presentaron antes este tipo de valores. Para 'year' podemos ver que son solo 24 valores nulos; por tanto, al ser una cantidad lo suficientemente pequeña, podemos eliminar estas filas de nuestro dataset.

```{r echo=TRUE}
cleanData <- dataset[!is.na(dataset$year),]
```

Sin embargo, 'number_users' presenta un mayor número de valores NA: un total de 488 observaciones. Por tanto, tendremos que analizar previamente su impacto junto al campo 'user_score', dada la relación entre ambos:

```{r echo=TRUE}
dataUsers <- cleanData %>% dplyr::filter(is.na(cleanData$number_users)) %>% dplyr::select(user_score, number_users)
dataUsers %>% head()
```
Número de filas presentes en dataUsers, es decir, número de observaciones que tienen valor NA en 'number_users':
```{r echo=TRUE}
dataUsers %>% nrow()
```
Puntuaciones únicas cuando number_users es NA:
```{r echo=TRUE}
dataUsers$user_score %>% table()
```
Puntuaciones únicas en el dataset limpio (hasta este paso):
```{r echo=TRUE}
cleanData$user_score %>% table()
```
Podemos comprobar que, cuando el número de usuarios ('number_users') posee valor NA, siguen existiendo puntuaciones asociadas a los usuarios; esto puede deberse a que la página no haya registrado el número de usuarios implicados en la valoración de la película, o que nuestra recogida de datos no ha extraído correctamente esta información; aparte, hay 380 observaciones que no poseen una puntuación indicada (tbd o "to be discussed") - también hemos comprobado si esto ocurre en el dataset limpiado hasta este punto (sin las películas con 'year' igual a NA), y siguen siendo 380 observaciones. 

Ante esta situación, hemos decidido asignar, por un lado, el valor 1 para 'number_users' en aquellas filas donde aparezca un valor distinto a "tbd"; por otro lado, el valor "tbd" de 'user_score' se intercambiará por el valor 0 para todos los casos, mientras que el campo 'number_users' será 0 cuando sea NA, dado que al seguir siendo bastantes datos (464 de 2572), su eliminación podría afectar a nuestro análisis.

```{r echo=TRUE}
cleanData$user_score[cleanData$user_score == "tbd"] <- 0.0
cleanData$number_users[is.na(cleanData$number_users)] <- 0
cleanData$user_score %>% table() %>% head()
cleanData$number_users %>% table() %>% head()
```

Para terminar con estos campos, pasaremos 'number_users' de "numeric" a "integer", es decir, cambiarlo de número decimal a número entero, dado que el número de usuarios no debería poder aceptar valores decimales; por otro lado, 'user_score' pasará de "character" a "integer" y multiplicaremos por 10, para dejarlo sobre 100 igual que metascore:

```{r echo=TRUE}
cleanData$number_users <- cleanData$number_users %>% as.integer()
cleanData$user_score   <- cleanData$user_score %>% as.integer()
cleanData$user_score   <- cleanData$user_score * 10
```


Tras la eliminación de las variables categóricas presentadas en el apartado de "Integración y selección de los datos de interés a analizar", comprobaremos si el resto presentan como valor la cadena vacía (""), y según la frecuencia de dicho valor, nos plantearemos si eliminar dichas filas o sustituirlas por un valor distinto:

```{r echo=TRUE}
colsToCheck <- c("rating", "duration", "must_see", "genre")
numberEmpty <- numeric(length(colsToCheck))

# Recorrer cada columna del dataset
for (i in seq_along(colsToCheck)) {
  # Verificar si hay valores de cadena vacía en la columna actual
  numberEmpty[i] <- sum(cleanData[[colsToCheck[i]]] == "")
}

# Mostrar el número de veces que aparece la cadena vacía para cada variable
names(numberEmpty) <- colsToCheck
print("Número de cadenas vacías ('') en las columnas categóricas:")
print(numberEmpty)
```
Podemos comprobar que existen cadenas vacías en las variables categóricas, por lo que, comenzando por 'rating', analizaremos sus valores y frecuencias para comprobar esas 325 observaciones que están vacías, así como otras consideraciones que puedan surgir durante el procesado y debamos tener en cuenta:

```{r echo=TRUE}
cleanData$rating %>% table() %>% sort(decreasing = T)
```

Se observa que hay multitud de calificaciones, desde clasificaciones de películas hasta clasificaciones propias de series de televisión (por ejemplo, TV-14), así como valores vacíos o equivalentes ("Not Rated"). Ante esta situación, y para facilitar su uso posterior, codificaremos esta variable combinando las clasificaciones por películas (ver referencia: https://www.motionpictures.org/film-ratings/) como de series de televisión (ver referencia: https://www.thetvboss.org/tv-ratings/); en caso de ser un valor que no esté incluido en ambas, se asignará un valor nuevo dependiendo del tipo que sea. La lista de codificación a una variable numérica será la siguiente:

* Valor 0: Not Rated, "", Unrated 
* Valor 1: G, TV-Y, TV-Y7, TV-Y7-FV, TV-G
* Valor 2: PG, TV-PG
* Valor 3: PG-13, TV-14
* Valor 4: R
* Valor 5: NC-17, TV-MA
* Valor 6: El resto de clasificaciones

Somos conscientes de que esto puede simplificar su categorización más de lo necesario, pero mejorará su uso para los algoritmos que apliquemos más tarde. Dicho esto, realizaremos el cambio a continuación - pero cabe decir que, primero, cambiaremos el valor "" por "Empty" y "Not Rated" por "Not_Rated" para poder emplear la función recode() sin errores:

```{r echo=TRUE}
cleanData$rating <- ifelse(cleanData$rating == "", "Empty", cleanData$rating)
cleanData$rating <- ifelse(cleanData$rating == "Not Rated", "Not_Rated", cleanData$rating)
cleanData$rating <- dplyr::recode(cleanData$rating,
                           "Not_Rated" = 0, "Empty" = 0, "Unrated" = 0,
                           "G"  = 1, "TV-Y"  = 1, "TV-Y7" = 1, "TV-Y7-FV" = 1, "TV-G" = 1,
                           "PG" = 2, "TV-PG" = 2,
                           "PG-13" = 3, "TV-14" = 3,
                           "R" = 4,
                           "NC-17" = 5, "TV-MA" = 5,
                           .default = 6) %>% as.integer()
```

Valores absolutos tras la codificación:
```{r echo=TRUE}
cleanData$rating %>% table()
```
Pesos por porcentajes tras la codificación:
```{r echo=TRUE}
cleanData$rating %>% table() %>% prop.table()*100
```
Lo siguiente es analizar el campo 'duration': esta variable recoge la duración de la película en un formato "XXh YYm", pudiendo no presentar el campo de la hora si la película es lo suficientemente corta. Además, solo presenta 54 filas cuyos valores estén vacíos, por lo que podamos eliminar dichas filas de nuestro análisis, y aprovecharemos por eliminar también los valores NA que podamos haber no detectado en este proceso de limpieza:
```{r echo=TRUE}
cleanData <- cleanData %>% filter(!is.na(duration) & duration != "")
```

El siguiente paso convertir la duración de las películas en un formato que sea más accesible, por lo que obtendremos el total de minutos de cada película en formato integer. Para ello, extraeremos la hora (si existe), la multiplicaremos por 60, y la sumaremos con el total de minutos, sustituyendo dicho resultado donde el valor original se encontraba:
```{r echo=TRUE}
cleanData <- cleanData %>% mutate(
                              duration = gsub(" ", "", duration),
                              duration = ifelse(is.na(str_extract(duration, "\\d+(?=h)")), 0, as.integer(str_extract(duration, "\\d+(?=h)"))) * 60 + as.integer(str_extract(duration,"\\d+(?=m)")))
```

Tras esto, comprobamos cómo son las estadísticas básicas de esta variable, ahora de tipo numérica:
```{r echo=TRUE}
cleanData %>%
    summarise(
        count = n(),                                 # Número de observaciones
        mean = mean(duration, na.rm = TRUE),         # Media
        sd = sd(duration, na.rm = TRUE),             # Desviación estándar
        min = min(duration, na.rm = TRUE),           # Mínimo
        q1 = quantile(duration, 0.25, na.rm = TRUE), # 1er cuartil
        median = median(duration, na.rm = TRUE),     # Mediana
        q3 = quantile(duration, 0.75, na.rm = TRUE), # 3er cuartil
        max = max(duration, na.rm = TRUE)            # Máximo
    )
```
Hemos podido comprobar que la película más corta dura 30 minutos, mientras que la más larga se extiende hasta los 776 minutos (tal duración puede deberse a que haya sido realmente una serie de televisión clasificada como película). Para facilitar nuestro análisis, dividiremos y codificaremos las películas según los cuartiles; es decir, si una película es superior a 30 minutos pero inferior a 93.25, se considerará muy corta; además, como la diferencia entre el tercer cuartil y el máximo es de gran magnitud, añadiremos un límite más que será el valor intermedio entre el q3 y el máximo: (776-121)/2= 327.5 ~ 328. El listado es el siguiente:

* Valor 0 (muy corta): 30 <= x < 93 
* Valor 1 (corta): 93 <= x < 105
* Valor 2 (normal): 105 <= x < 121
* Valor 3 (larga): 121 <= x < 328
* Valor 4 (muy larga): 328 <= x <= 776

```{r echo=TRUE}
# Utilizamos la función cut() para categorizar la duración de las películas según los límites indicados
cleanData$duration <- cut(cleanData$duration, breaks = c(30, 93, 105, 121, 328, 776), labels = FALSE, right = FALSE) -1 # -1 para empezar desde 0
cleanData$duration <- cleanData$duration %>% as.integer()

# Visualizamos la frecuencia de cada valor
cleanData$duration %>% table()
```
El siguiente campo a analizar es el género, el cual podemos observar que puede presentar muchas combinaciones:
```{r echo=TRUE}
cleanData$genre %>% head()
cleanData$genre %>% unique() %>% length()
```
Esto se debe a que pueden haber multitud de combinaciones de géneros; para facilitar nuestro análisis, comprobemos primero qué géneros existen de manera individual y con qué frecuencia aparecen:

```{r echo=TRUE}
cleanData$genre %>% paste(., collapse = ",") %>% strsplit(",") %>% unlist() %>% trimws() %>% table() %>% sort(decreasing = T)
cat("Número de géneros diferentes: ", cleanData$genre %>% paste(., collapse = ",") %>% strsplit(",") %>% unlist() %>% trimws() %>% table() %>% length())
```
Podemos observar que hay 25 géneros distintos, entre los que se incluyen géneros desconocidos ('Unknow'). Sin embargo, nuestro problema radica en que cada película puede poseer distintos géneros que la definan, así como un número variable de géneros, como se muestra a continuación:

```{r echo=TRUE}
# Generar una tabla de frecuencia
cleanData$genre %>% strsplit(",") %>% sapply(length) %>% table()
```

Se ha decidido crear diferentes columnas dentro del dataset de tipo "boleano" para indicar cada uno de los generos. Lo hacemos comprobando para cada una de las muestras si existe el genero(string) y asignando 0 o 1 en función si lo encuentra o no. Como eran muchos generos, se ha decidido mostrar solo los más frecuentes y el resto asignarlos a "others". 
Así, podremos analizar con más facilidad sobre este atributo:

```{r echo=TRUE}

cleanData$drama <- ifelse(grepl("Drama", cleanData$genre) == "TRUE", 1, 0) %>% as.integer()
cleanData$crime <- ifelse(grepl("Crime", cleanData$genre) == "TRUE", 1, 0) %>% as.integer()
cleanData$mystery <- ifelse(grepl("Mystery", cleanData$genre) == "TRUE", 1, 0) %>% as.integer()
cleanData$romance <- ifelse(grepl("Romance", cleanData$genre) == "TRUE", 1, 0) %>% as.integer()
cleanData$adventure <- ifelse(grepl("Adventure", cleanData$genre) == "TRUE", 1, 0) %>% as.integer()
cleanData$biography <- ifelse(grepl("Biography", cleanData$genre) == "TRUE", 1, 0) %>% as.integer()
cleanData$thriller <- ifelse(grepl("Thriller", cleanData$genre) == "TRUE", 1, 0) %>% as.integer()
cleanData$comedy <- ifelse(grepl("Comedy", cleanData$genre) == "TRUE", 1, 0) %>% as.integer()
cleanData$history <- ifelse(grepl("History", cleanData$genre) == "TRUE", 1, 0) %>% as.integer()
cleanData$documentary <- ifelse(grepl("Documentary", cleanData$genre) == "TRUE", 1, 0) %>% as.integer()
cleanData$others <- ifelse(grepl("Action|Game-Show|News|Film-Noir|Western|Musical|Sport|Horror|Sci-Fi|Animation|Family|Action|Fantasy|War|Music", cleanData$genre) == "TRUE", 1, 0) %>% as.integer()

```

A continuación eliminamos la columna "genre" porque ya no nos hace falta:
```{r echo=TRUE}
cleanData <- cleanData %>% select(-genre)
```

```{r echo=TRUE}
str(cleanData)
```

Indentificación y tratamiento de valores extremos:

```{r echo=TRUE}
g_caja<-boxplot(cleanData$duration, col="skyblue", frame.plot=F)

```

Comprobar si hay valores extremos (por ejemplo, en 'duration' ya se ve que hay 4 peliculas) con histogramas para visualizarlo




### REVISAR

Codificación de la variable objetivo, donde "True" es 1, y "False" será 0:

```{r echo=TRUE}
cleanData$must_see <- ifelse(cleanData$must_see == "True", 1, 0) %>% as.integer()
```

Por último, comprobaremos cuántas filas repetidas existen en el dataset y las eliminaremos, como se puede apreciar a continuación:

```{r echo=TRUE}
cat("Número de filas totales: ", cleanData %>% nrow())
cat("Número de filas repetidas: ", cleanData %>% duplicated() %>% sum())
cleanData <- cleanData %>% filter(!duplicated(.))
```

Estado final del dataset:
```{r echo=TRUE}
str(cleanData)
```
```{r echo=TRUE}
summary(cleanData)
head(cleanData)
```


## Análisis de los datos

TODO: 
Normalización de los datos numéricos (casi todos al final de la limpieza)
Un modelo supervisado
Un modelo no supervisado

(Representación visual en todo momento de los resultados)

Enunciado:
"1.Aplica un modelo supervisado y uno no supervisado a los datos y
comenta los resultados obtenidos.
2.Aplica una prueba por contraste de hipótesis. Ten en cuenta que algunas
de estas pruebas requieren verificar previamente la normalidad y
homocedasticidad de los datos"

## Conclusiones 

TODO:
Explicación de los resultados obtenidos

